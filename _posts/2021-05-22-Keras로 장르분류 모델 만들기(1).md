---
layout: post
title: Keras로 장르분류 모델 만들기
categories: Data Analysis
tags: 
 - Machine learning
---


### 0. 개요

웹소설 분석 사이트에서 6개월간 모인 데이터로 머신러닝을 해보기로 했다. 제목만 보고 장르를 분류하는 모델을 만들것이다. 
코드는 [이 블로그](https://somjang.tistory.com/entry/Keras%EA%B8%B0%EC%82%AC-%EC%A0%9C%EB%AA%A9%EC%9D%84-%EA%B0%80%EC%A7%80%EA%B3%A0-%EA%B8%8D%EC%A0%95-%EB%B6%80%EC%A0%95-%EC%A4%91%EB%A6%BD-%EB%B6%84%EB%A5%98%ED%95%98%EB%8A%94-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EC%96%B4%EB%B3%B4%EA%B8%B0)
를 참고했다. 

### 1. 장르별 데이터 확인


```python
import pandas as pd
```


```python
train_data = pd.read_csv('joara-tobe.csv', encoding='utf-8')
```


```python
genre = set(train_data['genre'])
label = list(genre)
```


```python
print(label)
```

    ['[역사]', '[패러디]', '[무협]', '[팬픽]', '[로맨스]', '[퓨전]', '[BL]', '[로맨스판타지]', '[GL]', '[라이트노벨]', '[판타지]']



```python
%matplotlib inline 
import matplotlib.pyplot as plt
```

    Matplotlib created a temporary config/cache directory at /tmp/matplotlib-4zvp6e6k because the default path (/home/ubuntu/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.



```python
from matplotlib import font_manager, rc
font_path = "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf"
font = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font)
```


```python
train_data['genre'].value_counts().plot(kind='bar')
```
    
![output_7_1](https://user-images.githubusercontent.com/63631604/119225397-86457f00-bb3e-11eb-8142-3da761790c33.png)


### 2. 데이터 전처리 작업


```python
import konlpy 
from konlpy.tag import Okt
import re

stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다', '이다']

okt = Okt() 
X_train = [] 
for sentence in train_data['title']: 
    sentence = re.sub(r'\<[^)]*\>', '', sentence)
    temp_X = [] 
    temp_X = okt.morphs(sentence, stem=True) # 토큰화 
    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거 
    X_train.append(temp_X) 
```


```python
X_train
```

    [['사모', '그립다'],
     ['시한', '부', '황제', '주치의'],
     ['슬로우', '러브', '어택'],
     ['내', '죽다', '줄', '알', '고', '남편', '폭', '군', '되다', '..'],
     ['쉿', ',', '황자', '비다', '상담', '중', '.'],
     ...]


```python
from keras.preprocessing.text import Tokenizer
max_words = 35000
tokenizer = Tokenizer(num_words = max_words)
tokenizer.fit_on_texts(X_train)
X_train = tokenizer.texts_to_sequences(X_train)
```


```python
X_train
```




    [[1108, 1109],
     [54, 47, 103, 241],
     [594, 198, 595],
     [18, 81, 310, 786, 76, 66, 121, 93, 37, 3],
     [55, 142, 199, 104],
     ...]




```python
print("제목의 최대 길이 : ", max(len(l) for l in X_train)) 
print("제목의 평균 길이 : ", sum(map(len, X_train))/ len(X_train)) 
plt.hist([len(s) for s in X_train], bins=50) 
plt.xlabel('length of Data')
plt.ylabel('number of Data') 
plt.show()
```

    제목의 최대 길이 :  15
    제목의 평균 길이 :  5.57841726618705
    
![output_13_1](https://user-images.githubusercontent.com/63631604/119225399-880f4280-bb3e-11eb-94d0-0a33da3d7503.png)



    
